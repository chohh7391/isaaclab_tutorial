{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a7032a",
   "metadata": {},
   "source": [
    "# Joint Action (Parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from collections.abc import Sequence\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import omni.log\n",
    "\n",
    "import isaaclab.utils.string as string_utils\n",
    "from isaaclab.assets.articulation import Articulation\n",
    "from isaaclab.managers.action_manager import ActionTerm\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from isaaclab.envs import ManagerBasedEnv\n",
    "    from isaaclab.envs.utils.io_descriptors import GenericActionIODescriptor\n",
    "\n",
    "    from . import actions_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c30c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointAction(ActionTerm):\n",
    "    r\"\"\"Base class for joint actions.\n",
    "\n",
    "    This action term performs pre-processing of the raw actions using affine transformations (scale and offset).\n",
    "    These transformations can be configured to be applied to a subset of the articulation's joints.\n",
    "\n",
    "    Mathematically, the action term is defined as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       \\text{action} = \\text{offset} + \\text{scaling} \\times \\text{input action}\n",
    "\n",
    "    where :math:`\\text{action}` is the action that is sent to the articulation's actuated joints, :math:`\\text{offset}`\n",
    "    is the offset applied to the input action, :math:`\\text{scaling}` is the scaling applied to the input\n",
    "    action, and :math:`\\text{input action}` is the input action from the user.\n",
    "\n",
    "    Based on above, this kind of action transformation ensures that the input and output actions are in the same\n",
    "    units and dimensions. The child classes of this action term can then map the output action to a specific\n",
    "    desired command of the articulation's joints (e.g. position, velocity, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    cfg: actions_cfg.JointActionCfg\n",
    "    \"\"\"The configuration of the action term.\"\"\"\n",
    "    _asset: Articulation\n",
    "    \"\"\"The articulation asset on which the action term is applied.\"\"\"\n",
    "    _scale: torch.Tensor | float\n",
    "    \"\"\"The scaling factor applied to the input action.\"\"\"\n",
    "    _offset: torch.Tensor | float\n",
    "    \"\"\"The offset applied to the input action.\"\"\"\n",
    "    _clip: torch.Tensor\n",
    "    \"\"\"The clip applied to the input action.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: actions_cfg.JointActionCfg, env: ManagerBasedEnv) -> None:\n",
    "        # initialize the action term\n",
    "        super().__init__(cfg, env)\n",
    "\n",
    "        # resolve the joints over which the action term is applied\n",
    "        self._joint_ids, self._joint_names = self._asset.find_joints(\n",
    "            self.cfg.joint_names, preserve_order=self.cfg.preserve_order\n",
    "        )\n",
    "        self._num_joints = len(self._joint_ids)\n",
    "        # log the resolved joint names for debugging\n",
    "        omni.log.info(\n",
    "            f\"Resolved joint names for the action term {self.__class__.__name__}:\"\n",
    "            f\" {self._joint_names} [{self._joint_ids}]\"\n",
    "        )\n",
    "\n",
    "        # Avoid indexing across all joints for efficiency\n",
    "        if self._num_joints == self._asset.num_joints and not self.cfg.preserve_order:\n",
    "            self._joint_ids = slice(None)\n",
    "\n",
    "        # create tensors for raw and processed actions\n",
    "        self._raw_actions = torch.zeros(self.num_envs, self.action_dim, device=self.device)\n",
    "        self._processed_actions = torch.zeros_like(self.raw_actions)\n",
    "\n",
    "        # parse scale\n",
    "        if isinstance(cfg.scale, (float, int)):\n",
    "            self._scale = float(cfg.scale)\n",
    "        elif isinstance(cfg.scale, dict):\n",
    "            self._scale = torch.ones(self.num_envs, self.action_dim, device=self.device)\n",
    "            # resolve the dictionary config\n",
    "            index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.scale, self._joint_names)\n",
    "            self._scale[:, index_list] = torch.tensor(value_list, device=self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scale type: {type(cfg.scale)}. Supported types are float and dict.\")\n",
    "        # parse offset\n",
    "        if isinstance(cfg.offset, (float, int)):\n",
    "            self._offset = float(cfg.offset)\n",
    "        elif isinstance(cfg.offset, dict):\n",
    "            self._offset = torch.zeros_like(self._raw_actions)\n",
    "            # resolve the dictionary config\n",
    "            index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.offset, self._joint_names)\n",
    "            self._offset[:, index_list] = torch.tensor(value_list, device=self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported offset type: {type(cfg.offset)}. Supported types are float and dict.\")\n",
    "        # parse clip\n",
    "        if self.cfg.clip is not None:\n",
    "            if isinstance(cfg.clip, dict):\n",
    "                self._clip = torch.tensor([[-float(\"inf\"), float(\"inf\")]], device=self.device).repeat(\n",
    "                    self.num_envs, self.action_dim, 1\n",
    "                )\n",
    "                index_list, _, value_list = string_utils.resolve_matching_names_values(self.cfg.clip, self._joint_names)\n",
    "                self._clip[:, index_list] = torch.tensor(value_list, device=self.device)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported clip type: {type(cfg.clip)}. Supported types are dict.\")\n",
    "\n",
    "    \"\"\"\n",
    "    Properties.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def action_dim(self) -> int:\n",
    "        return self._num_joints\n",
    "\n",
    "    @property\n",
    "    def raw_actions(self) -> torch.Tensor:\n",
    "        return self._raw_actions\n",
    "\n",
    "    @property\n",
    "    def processed_actions(self) -> torch.Tensor:\n",
    "        return self._processed_actions\n",
    "\n",
    "    @property\n",
    "    def IO_descriptor(self) -> GenericActionIODescriptor:\n",
    "        \"\"\"The IO descriptor of the action term.\n",
    "\n",
    "        This descriptor is used to describe the action term of the joint action.\n",
    "        It adds the following information to the base descriptor:\n",
    "        - joint_names: The names of the joints.\n",
    "        - scale: The scale of the action term.\n",
    "        - offset: The offset of the action term.\n",
    "        - clip: The clip of the action term.\n",
    "\n",
    "        Returns:\n",
    "            The IO descriptor of the action term.\n",
    "        \"\"\"\n",
    "        super().IO_descriptor\n",
    "        self._IO_descriptor.shape = (self.action_dim,)\n",
    "        self._IO_descriptor.dtype = str(self.raw_actions.dtype)\n",
    "        self._IO_descriptor.action_type = \"JointAction\"\n",
    "        self._IO_descriptor.joint_names = self._joint_names\n",
    "        self._IO_descriptor.scale = self._scale\n",
    "        # This seems to be always [4xNum_joints] IDK why. Need to check.\n",
    "        if isinstance(self._offset, torch.Tensor):\n",
    "            self._IO_descriptor.offset = self._offset[0].detach().cpu().numpy().tolist()\n",
    "        else:\n",
    "            self._IO_descriptor.offset = self._offset\n",
    "        # FIXME: This is not correct. Add list support.\n",
    "        if self.cfg.clip is not None:\n",
    "            if isinstance(self._clip, torch.Tensor):\n",
    "                self._IO_descriptor.clip = self._clip[0].detach().cpu().numpy().tolist()\n",
    "            else:\n",
    "                self._IO_descriptor.clip = self._clip\n",
    "        else:\n",
    "            self._IO_descriptor.clip = None\n",
    "        return self._IO_descriptor\n",
    "\n",
    "    \"\"\"\n",
    "    Operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_actions(self, actions: torch.Tensor):\n",
    "        # store the raw actions\n",
    "        self._raw_actions[:] = actions\n",
    "        # apply the affine transformations\n",
    "        self._processed_actions = self._raw_actions * self._scale + self._offset\n",
    "        # clip actions\n",
    "        if self.cfg.clip is not None:\n",
    "            self._processed_actions = torch.clamp(\n",
    "                self._processed_actions, min=self._clip[:, :, 0], max=self._clip[:, :, 1]\n",
    "            )\n",
    "\n",
    "    def reset(self, env_ids: Sequence[int] | None = None) -> None:\n",
    "        self._raw_actions[env_ids] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802007c6",
   "metadata": {},
   "source": [
    "## Joint Actions (Child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointPositionAction(JointAction):\n",
    "    \"\"\"Joint action term that applies the processed actions to the articulation's joints as position commands.\"\"\"\n",
    "\n",
    "    cfg: actions_cfg.JointPositionActionCfg\n",
    "    \"\"\"The configuration of the action term.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: actions_cfg.JointPositionActionCfg, env: ManagerBasedEnv):\n",
    "        # initialize the action term\n",
    "        super().__init__(cfg, env)\n",
    "        # use default joint positions as offset\n",
    "        if cfg.use_default_offset:\n",
    "            self._offset = self._asset.data.default_joint_pos[:, self._joint_ids].clone()\n",
    "\n",
    "    def apply_actions(self):\n",
    "        # set position targets\n",
    "        self._asset.set_joint_position_target(self.processed_actions, joint_ids=self._joint_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeJointPositionAction(JointAction):\n",
    "    r\"\"\"Joint action term that applies the processed actions to the articulation's joints as relative position commands.\n",
    "\n",
    "    Unlike :class:`JointPositionAction`, this action term applies the processed actions as relative position commands.\n",
    "    This means that the processed actions are added to the current joint positions of the articulation's joints\n",
    "    before being sent as position commands.\n",
    "\n",
    "    This means that the action applied at every step is:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "         \\text{applied action} = \\text{current joint positions} + \\text{processed actions}\n",
    "\n",
    "    where :math:`\\text{current joint positions}` are the current joint positions of the articulation's joints.\n",
    "    \"\"\"\n",
    "\n",
    "    cfg: actions_cfg.RelativeJointPositionActionCfg\n",
    "    \"\"\"The configuration of the action term.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: actions_cfg.RelativeJointPositionActionCfg, env: ManagerBasedEnv):\n",
    "        # initialize the action term\n",
    "        super().__init__(cfg, env)\n",
    "        # use zero offset for relative position\n",
    "        if cfg.use_zero_offset:\n",
    "            self._offset = 0.0\n",
    "\n",
    "    def apply_actions(self):\n",
    "        # add current joint positions to the processed actions\n",
    "        current_actions = self.processed_actions + self._asset.data.joint_pos[:, self._joint_ids]\n",
    "        # set position targets\n",
    "        self._asset.set_joint_position_target(current_actions, joint_ids=self._joint_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointVelocityAction(JointAction):\n",
    "    \"\"\"Joint action term that applies the processed actions to the articulation's joints as velocity commands.\"\"\"\n",
    "\n",
    "    cfg: actions_cfg.JointVelocityActionCfg\n",
    "    \"\"\"The configuration of the action term.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: actions_cfg.JointVelocityActionCfg, env: ManagerBasedEnv):\n",
    "        # initialize the action term\n",
    "        super().__init__(cfg, env)\n",
    "        # use default joint velocity as offset\n",
    "        if cfg.use_default_offset:\n",
    "            self._offset = self._asset.data.default_joint_vel[:, self._joint_ids].clone()\n",
    "\n",
    "    def apply_actions(self):\n",
    "        # set joint velocity targets\n",
    "        self._asset.set_joint_velocity_target(self.processed_actions, joint_ids=self._joint_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointEffortAction(JointAction):\n",
    "    \"\"\"Joint action term that applies the processed actions to the articulation's joints as effort commands.\"\"\"\n",
    "\n",
    "    cfg: actions_cfg.JointEffortActionCfg\n",
    "    \"\"\"The configuration of the action term.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: actions_cfg.JointEffortActionCfg, env: ManagerBasedEnv):\n",
    "        super().__init__(cfg, env)\n",
    "\n",
    "    def apply_actions(self):\n",
    "        # set joint effort targets\n",
    "        self._asset.set_joint_effort_target(self.processed_actions, joint_ids=self._joint_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5953896",
   "metadata": {},
   "source": [
    "# Task Space Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9733413",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
