{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from isaaclab.app import AppLauncher\n",
    "\n",
    "# add argparse arguments\n",
    "parser = argparse.ArgumentParser(description=\"This script demonstrates how to use the camera sensor.\")\n",
    "parser.add_argument(\n",
    "    \"--draw\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Draw the pointcloud from camera at index specified by ``--camera_id``.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Save the data from camera at index specified by ``--camera_id``.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--camera_id\",\n",
    "    type=int,\n",
    "    choices={0, 1},\n",
    "    default=0,\n",
    "    help=(\n",
    "        \"The camera ID to use for displaying points or saving the camera data. Default is 0.\"\n",
    "        \" The viewport will always initialize with the perspective of camera 0.\"\n",
    "    ),\n",
    ")\n",
    "# append AppLauncher cli args\n",
    "AppLauncher.add_app_launcher_args(parser)\n",
    "# parse the arguments\n",
    "args_cli = parser.parse_args()\n",
    "\n",
    "# launch omniverse app\n",
    "app_launcher = AppLauncher(args_cli)\n",
    "simulation_app = app_launcher.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import isaacsim.core.utils.prims as prim_utils\n",
    "import omni.replicator.core as rep\n",
    "\n",
    "import isaaclab.sim as sim_utils\n",
    "from isaaclab.assets import RigidObject, RigidObjectCfg\n",
    "from isaaclab.markers import VisualizationMarkers\n",
    "from isaaclab.markers.config import RAY_CASTER_MARKER_CFG\n",
    "from isaaclab.sensors.camera import Camera, CameraCfg\n",
    "from isaaclab.sensors.camera.utils import create_pointcloud_from_depth\n",
    "from isaaclab.utils import convert_dict_to_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sensor() -> Camera:\n",
    "    \"\"\"Defines the camera sensor to add to the scene.\"\"\"\n",
    "    # Setup camera sensor\n",
    "    # In contrast to the ray-cast camera, we spawn the prim at these locations.\n",
    "    # This means the camera sensor will be attached to these prims.\n",
    "    prim_utils.create_prim(\"/World/Origin_00\", \"Xform\")\n",
    "    prim_utils.create_prim(\"/World/Origin_01\", \"Xform\")\n",
    "    camera_cfg = CameraCfg(\n",
    "        prim_path=\"/World/Origin_.*/CameraSensor\",\n",
    "        update_period=0,\n",
    "        height=480,\n",
    "        width=640,\n",
    "        data_types=[\n",
    "            \"rgb\",\n",
    "            \"distance_to_image_plane\",\n",
    "            \"normals\",\n",
    "            \"semantic_segmentation\",\n",
    "            \"instance_segmentation_fast\",\n",
    "            \"instance_id_segmentation_fast\",\n",
    "        ],\n",
    "        colorize_semantic_segmentation=True,\n",
    "        colorize_instance_id_segmentation=True,\n",
    "        colorize_instance_segmentation=True,\n",
    "        spawn=sim_utils.PinholeCameraCfg(\n",
    "            focal_length=24.0, focus_distance=400.0, horizontal_aperture=20.955, clipping_range=(0.1, 1.0e5)\n",
    "        ),\n",
    "    )\n",
    "    # Create camera\n",
    "    camera = Camera(cfg=camera_cfg)\n",
    "\n",
    "    return camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_scene() -> dict:\n",
    "    \"\"\"Design the scene.\"\"\"\n",
    "    # Populate scene\n",
    "    # -- Ground-plane\n",
    "    cfg = sim_utils.GroundPlaneCfg()\n",
    "    cfg.func(\"/World/defaultGroundPlane\", cfg)\n",
    "    # -- Lights\n",
    "    cfg = sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))\n",
    "    cfg.func(\"/World/Light\", cfg)\n",
    "\n",
    "    # Create a dictionary for the scene entities\n",
    "    scene_entities = {}\n",
    "\n",
    "    # Xform to hold objects\n",
    "    prim_utils.create_prim(\"/World/Objects\", \"Xform\")\n",
    "    # Random objects\n",
    "    for i in range(8):\n",
    "        # sample random position\n",
    "        position = np.random.rand(3) - np.asarray([0.05, 0.05, -1.0])\n",
    "        position *= np.asarray([1.5, 1.5, 0.5])\n",
    "        # sample random color\n",
    "        color = (random.random(), random.random(), random.random())\n",
    "        # choose random prim type\n",
    "        prim_type = random.choice([\"Cube\", \"Cone\", \"Cylinder\"])\n",
    "        common_properties = {\n",
    "            \"rigid_props\": sim_utils.RigidBodyPropertiesCfg(),\n",
    "            \"mass_props\": sim_utils.MassPropertiesCfg(mass=5.0),\n",
    "            \"collision_props\": sim_utils.CollisionPropertiesCfg(),\n",
    "            \"visual_material\": sim_utils.PreviewSurfaceCfg(diffuse_color=color, metallic=0.5),\n",
    "            \"semantic_tags\": [(\"class\", prim_type)],\n",
    "        }\n",
    "        if prim_type == \"Cube\":\n",
    "            shape_cfg = sim_utils.CuboidCfg(size=(0.25, 0.25, 0.25), **common_properties)\n",
    "        elif prim_type == \"Cone\":\n",
    "            shape_cfg = sim_utils.ConeCfg(radius=0.1, height=0.25, **common_properties)\n",
    "        elif prim_type == \"Cylinder\":\n",
    "            shape_cfg = sim_utils.CylinderCfg(radius=0.25, height=0.25, **common_properties)\n",
    "        # Rigid Object\n",
    "        obj_cfg = RigidObjectCfg(\n",
    "            prim_path=f\"/World/Objects/Obj_{i:02d}\",\n",
    "            spawn=shape_cfg,\n",
    "            init_state=RigidObjectCfg.InitialStateCfg(pos=position),\n",
    "        )\n",
    "        scene_entities[f\"rigid_object{i}\"] = RigidObject(cfg=obj_cfg)\n",
    "\n",
    "    # Sensors\n",
    "    camera = define_sensor()\n",
    "\n",
    "    # return the scene information\n",
    "    scene_entities[\"camera\"] = camera\n",
    "    return scene_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulator(sim: sim_utils.SimulationContext, scene_entities: dict):\n",
    "    \"\"\"Run the simulator.\"\"\"\n",
    "    # extract entities for simplified notation\n",
    "    camera: Camera = scene_entities[\"camera\"]\n",
    "\n",
    "    # Create replicator writer\n",
    "    output_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"output\", \"camera\")\n",
    "    rep_writer = rep.BasicWriter(\n",
    "        output_dir=output_dir,\n",
    "        frame_padding=0,\n",
    "        colorize_instance_id_segmentation=camera.cfg.colorize_instance_id_segmentation,\n",
    "        colorize_instance_segmentation=camera.cfg.colorize_instance_segmentation,\n",
    "        colorize_semantic_segmentation=camera.cfg.colorize_semantic_segmentation,\n",
    "    )\n",
    "\n",
    "    # Camera positions, targets, orientations\n",
    "    camera_positions = torch.tensor([[2.5, 2.5, 2.5], [-2.5, -2.5, 2.5]], device=sim.device)\n",
    "    camera_targets = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], device=sim.device)\n",
    "    # These orientations are in ROS-convention, and will position the cameras to view the origin\n",
    "    camera_orientations = torch.tensor(  # noqa: F841\n",
    "        [[-0.1759, 0.3399, 0.8205, -0.4247], [-0.4247, 0.8205, -0.3399, 0.1759]], device=sim.device\n",
    "    )\n",
    "\n",
    "    # Set pose: There are two ways to set the pose of the camera.\n",
    "    # -- Option-1: Set pose using view\n",
    "    camera.set_world_poses_from_view(camera_positions, camera_targets)\n",
    "    # -- Option-2: Set pose using ROS\n",
    "    # camera.set_world_poses(camera_positions, camera_orientations, convention=\"ros\")\n",
    "\n",
    "    # Index of the camera to use for visualization and saving\n",
    "    camera_index = args_cli.camera_id\n",
    "\n",
    "    # Create the markers for the --draw option outside of is_running() loop\n",
    "    if sim.has_gui() and args_cli.draw:\n",
    "        cfg = RAY_CASTER_MARKER_CFG.replace(prim_path=\"/Visuals/CameraPointCloud\")\n",
    "        cfg.markers[\"hit\"].radius = 0.002\n",
    "        pc_markers = VisualizationMarkers(cfg)\n",
    "\n",
    "    # Simulate physics\n",
    "    while simulation_app.is_running():\n",
    "        # Step simulation\n",
    "        sim.step()\n",
    "        # Update camera data\n",
    "        camera.update(dt=sim.get_physics_dt())\n",
    "\n",
    "        # Print camera info\n",
    "        print(camera)\n",
    "        if \"rgb\" in camera.data.output.keys():\n",
    "            print(\"Received shape of rgb image        : \", camera.data.output[\"rgb\"].shape)\n",
    "        if \"distance_to_image_plane\" in camera.data.output.keys():\n",
    "            print(\"Received shape of depth image      : \", camera.data.output[\"distance_to_image_plane\"].shape)\n",
    "        if \"normals\" in camera.data.output.keys():\n",
    "            print(\"Received shape of normals          : \", camera.data.output[\"normals\"].shape)\n",
    "        if \"semantic_segmentation\" in camera.data.output.keys():\n",
    "            print(\"Received shape of semantic segm.   : \", camera.data.output[\"semantic_segmentation\"].shape)\n",
    "        if \"instance_segmentation_fast\" in camera.data.output.keys():\n",
    "            print(\"Received shape of instance segm.   : \", camera.data.output[\"instance_segmentation_fast\"].shape)\n",
    "        if \"instance_id_segmentation_fast\" in camera.data.output.keys():\n",
    "            print(\"Received shape of instance id segm.: \", camera.data.output[\"instance_id_segmentation_fast\"].shape)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "        # Extract camera data\n",
    "        if args_cli.save:\n",
    "            # Save images from camera at camera_index\n",
    "            # note: BasicWriter only supports saving data in numpy format, so we need to convert the data to numpy.\n",
    "            single_cam_data = convert_dict_to_backend(\n",
    "                {k: v[camera_index] for k, v in camera.data.output.items()}, backend=\"numpy\"\n",
    "            )\n",
    "\n",
    "            # Extract the other information\n",
    "            single_cam_info = camera.data.info[camera_index]\n",
    "\n",
    "            # Pack data back into replicator format to save them using its writer\n",
    "            rep_output = {\"annotators\": {}}\n",
    "            for key, data, info in zip(single_cam_data.keys(), single_cam_data.values(), single_cam_info.values()):\n",
    "                if info is not None:\n",
    "                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data, **info}}\n",
    "                else:\n",
    "                    rep_output[\"annotators\"][key] = {\"render_product\": {\"data\": data}}\n",
    "            # Save images\n",
    "            # Note: We need to provide On-time data for Replicator to save the images.\n",
    "            rep_output[\"trigger_outputs\"] = {\"on_time\": camera.frame[camera_index]}\n",
    "            rep_writer.write(rep_output)\n",
    "\n",
    "        # Draw pointcloud if there is a GUI and --draw has been passed\n",
    "        if sim.has_gui() and args_cli.draw and \"distance_to_image_plane\" in camera.data.output.keys():\n",
    "            # Derive pointcloud from camera at camera_index\n",
    "            pointcloud = create_pointcloud_from_depth(\n",
    "                intrinsic_matrix=camera.data.intrinsic_matrices[camera_index],\n",
    "                depth=camera.data.output[\"distance_to_image_plane\"][camera_index],\n",
    "                position=camera.data.pos_w[camera_index],\n",
    "                orientation=camera.data.quat_w_ros[camera_index],\n",
    "                device=sim.device,\n",
    "            )\n",
    "\n",
    "            # In the first few steps, things are still being instanced and Camera.data\n",
    "            # can be empty. If we attempt to visualize an empty pointcloud it will crash\n",
    "            # the sim, so we check that the pointcloud is not empty.\n",
    "            if pointcloud.size()[0] > 0:\n",
    "                pc_markers.visualize(translations=pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a99db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function.\"\"\"\n",
    "    # Load simulation context\n",
    "    sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)\n",
    "    sim = sim_utils.SimulationContext(sim_cfg)\n",
    "    # Set main camera\n",
    "    sim.set_camera_view([2.5, 2.5, 2.5], [0.0, 0.0, 0.0])\n",
    "    # Design scene\n",
    "    scene_entities = design_scene()\n",
    "    # Play simulator\n",
    "    sim.reset()\n",
    "    # Now we are ready!\n",
    "    print(\"[INFO]: Setup complete...\")\n",
    "    # Run simulator\n",
    "    run_simulator(sim, scene_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # run the main function\n",
    "    main()\n",
    "    # close sim app\n",
    "    simulation_app.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
